{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the i2b2 text report using cTAKES. Ensure that cTAKES is installed locally and that the text report is located within the cTAKES folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "class CTakesProcessor:\n",
    "    def __init__(self, ctakes_dir, input_dir, output_dir, pipeline_key):\n",
    "        self.ctakes_dir = ctakes_dir\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.pipeline_key = pipeline_key\n",
    "\n",
    "    def run_ctakes_pipeline(self):\n",
    "        command = fr'{self.ctakes_dir}\\bin\\runClinicalPipeline -i {self.input_dir}\\ --xmiOut {self.output_dir}\\ --key {self.pipeline_key}'\n",
    "        subprocess.run(command, shell=True, cwd=self.ctakes_dir)\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    ctakes_dir = r'C:/apache-ctakes-4.0.0.1/'\n",
    "    input_dir = r'C:/apache-ctakes-4.0.0.1/consolidated_text_reports_training_data'\n",
    "    output_dir = r'C:/apache-ctakes-4.0.0.1/consolidated_text_reports_training_data_cTAKES_processing'\n",
    "    pipeline_key = 'efd9c726-5226-43c1-8cb1-c5ac40bae98c'\n",
    "\n",
    "    ctakes_processor = CTakesProcessor(ctakes_dir, input_dir, output_dir, pipeline_key)\n",
    "    ctakes_processor.run_ctakes_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert cTAKES XMI output to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xmltodict\n",
    "import json\n",
    "\n",
    "def convert_xml_to_json(xml_file_path, json_file_path):\n",
    "    with open(xml_file_path, encoding='utf-8') as xml_file:\n",
    "        data_dict = xmltodict.parse(xml_file.read())\n",
    "    json_data = json.dumps(data_dict)\n",
    "\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "        json_file.write(json_data)\n",
    "\n",
    "def convert_all_xml_to_json(xml_folder_path, json_folder_path):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(json_folder_path):\n",
    "        os.makedirs(json_folder_path)\n",
    "\n",
    "    # Convert each XML file to JSON\n",
    "    for filename in os.listdir(xml_folder_path):\n",
    "        if filename.endswith(\".xmi\"):\n",
    "            xml_file_path = os.path.join(xml_folder_path, filename)\n",
    "            # removing the extension without adding a new one\n",
    "            json_file_path = os.path.join(json_folder_path, os.path.splitext(filename)[0])\n",
    "            convert_xml_to_json(xml_file_path, json_file_path)\n",
    "\n",
    "# usage\n",
    "xml_folder_path = \"./i2b2_2010_VA_training_data/i2b2_text_reports_cTAKES_processing\"\n",
    "json_folder_path = \"./i2b2_2010_VA_training_data/i2b2_text_reports_cTAKES_processing/json_output\"\n",
    "\n",
    "convert_all_xml_to_json(xml_folder_path, json_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract required information from JSON output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\018636330_DH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\026350193_RWH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\037945397_RWH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\044687343_ELMVH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\060376519_DH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\095889687_WGH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\101407944_PUMC.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\105732749.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\115026438_SC.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\130959255.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\134300717.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\143748600_SC.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\145980160.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\156406283.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\176318078_a.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\176746010_WGH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\188543380.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\194442600_RWH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\212512774_WGH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\223159990.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\245096078.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\245317863_WGH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\262182942.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\262912613.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\270045381.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\274230067_EH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\284487129.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\289811204.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\297228405_DH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\306968218_YC.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\320422564.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\332803550.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\333521954.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\337702516_WGH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\346176858_SC.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\348301810.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\351853846_WGH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\373254497_PUMC.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\384729825.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\388755206_SC.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\402389409_WGH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\405507617.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\405868244_YC.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\412141256.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\424729395_DH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\425680098_SC.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\433651389.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\455343475_PUMC.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\493597270.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\498710998.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\500472963.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\503651854_WGH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\517414339.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\523704694.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\544907529_RWH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\555509347_PUMC.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\574700124_RWH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\596437842.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\598403789_DH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\614746156.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\622086964.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\627258104.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\638157550_SC.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\641557794_WGH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\655358166_WGH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\688127038_EH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\693008750.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\699905656_SC.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\708739405_DH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\723989226.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\745431641_WGH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\745701560_WGH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\767751445_ELMVH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\779878634.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\788268693_DH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\814743340_RWH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\817406016_RWH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\825330116.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\837898389.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\839999049_YC.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\843566350_RWH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\853262744.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\869436718_SC.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\879492218_YC.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\888428725_RWH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\891864133_RWH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\910458031.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\915093496_RWH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\917989835_RWH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\920798564.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\932057504_DH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\950452368.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\959086752.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\965367286_WGH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\974381789.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\979440029_RWH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\989519730_WGH.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-105.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-106.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-107.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-108.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-109.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-121.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-122.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-123.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-124.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-13.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-14.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-140.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-141.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-142.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-143.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-144.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-15.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-16.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-17.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-175.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-176.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-177.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-178.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-179.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-18.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-19.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-20.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-21.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-22.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-23.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-24.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-25.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-26.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-27.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-28.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-29.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-30.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-31.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-32.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-33.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-34.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-35.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-36.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-37.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-38.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-45.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-46.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-47.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-48.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-49.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-50.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-51.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-52.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-53.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-54.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-55.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-56.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-58.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-59.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-65.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-66.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-67.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-68.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-69.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-70.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-71.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-73.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-74.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-80.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-81.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-82.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-83.txt\n",
      "./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output\\record-84.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Source directory containing the files to be parsed\n",
    "source_dir = \"./i2b2_2010_VA_training_data/i2b2_text_reports_cTAKES_processing/json_output\"\n",
    "\n",
    "# Destination directory to save the parsed files\n",
    "destination_dir = \"./i2b2_2010_VA_training_data/i2b2_text_reports_cTAKES_processing/json_output/parsed\"\n",
    "\n",
    "# Ensure destination directory exists\n",
    "if not os.path.exists(destination_dir):\n",
    "    os.makedirs(destination_dir)\n",
    "    \n",
    "\n",
    "# Iterate over each file in the source directory\n",
    "for file_name in os.listdir(source_dir):\n",
    "    file_path = os.path.join(source_dir, file_name)\n",
    "\n",
    "    # Check if the file is a .txt file\n",
    "    if os.path.isfile(file_path) and file_path.endswith(\".txt\"):\n",
    "        # Load the JSON data from the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "            print(file_path)\n",
    "            data = json.load(json_file)\n",
    "            \n",
    "        document_parser = {\n",
    "            \"UUID\":[],\n",
    "            \"statement\":[],\n",
    "            \"clinical_mention\": {\n",
    "                \"textsem:SignSymptomMention\": [],\n",
    "                \"textsem:AnatomicalSiteMention\": [],\n",
    "                \"textsem:DiseaseDisorderMention\": [],\n",
    "                \"textsem:ProcedureMention\": [],\n",
    "                \"textsem:MedicationMention\": [],\n",
    "                \"textsem:LabMention\": []\n",
    "                }\n",
    "            }\n",
    "        xmi_data = data.get(\"xmi:XMI\", {})\n",
    "        \n",
    "        def process_umls_concepts(umls_concept_data):\n",
    "            umls_concepts = {}\n",
    "            if isinstance(umls_concept_data, dict):  # Single UMLS Concept\n",
    "                xmi_id = umls_concept_data.get(\"@xmi:id\", None)\n",
    "                if xmi_id is not None:\n",
    "                    umls_concepts[xmi_id] = {\n",
    "                        \"cui\": umls_concept_data.get(\"@cui\", \"\"),\n",
    "                        \"tui\": umls_concept_data.get(\"@tui\", \"\"),\n",
    "                        \"preferredText\": umls_concept_data.get(\"@preferredText\", \"\")\n",
    "                        }\n",
    "            elif isinstance(umls_concept_data, list):  # List of UMLS Concepts\n",
    "                for concept in umls_concept_data:\n",
    "                    xmi_id = concept.get(\"@xmi:id\", None)\n",
    "                    if xmi_id is not None:\n",
    "                        umls_concepts[xmi_id] = {\n",
    "                            \"cui\": concept.get(\"@cui\", \"\"),\n",
    "                            \"tui\": concept.get(\"@tui\", \"\"),\n",
    "                            \"preferredText\": concept.get(\"@preferredText\", \"\")\n",
    "                            }\n",
    "            return umls_concepts\n",
    "\n",
    "        # # Extract UMLS Concepts data and process it\n",
    "        umls_concepts_data = data.get(\"xmi:XMI\", {}).get(\"refsem:UmlsConcept\", {})\n",
    "        umls_concepts = process_umls_concepts(umls_concepts_data)\n",
    "        \n",
    "        for mention_type, mentions_list in xmi_data.items():\n",
    "            if mention_type.startswith(\"structured:DocumentID\"):\n",
    "                UUID = mentions_list.get(\"@documentID\")\n",
    "                document_parser[\"UUID\"].append(UUID)\n",
    "            elif mention_type.startswith(\"cas:Sofa\"):\n",
    "                sofa = mentions_list.get(\"@sofaString\")\n",
    "                document_parser[\"statement\"].append(sofa)\n",
    "            elif mention_type.startswith(\"textsem:\"):\n",
    "                # Check if the mention type exists in clinical_mention\n",
    "                if mention_type in document_parser['clinical_mention']:\n",
    "                    if isinstance(mentions_list, list):\n",
    "                        for mention in mentions_list:\n",
    "                            mention_info = {  # Extract necessary information from each mention\n",
    "                                \"beginOffset\": mention.get(\"@begin\", \"\"),\n",
    "                                \"endOffset\": mention.get(\"@end\", \"\"),\n",
    "                                \"ontologyConceptArr\": mention.get(\"@ontologyConceptArr\", \"\"),\n",
    "                                \"confidence\": mention.get(\"@confidence\",\"\"),\n",
    "                                \"polarity\": mention.get(\"@polarity\", \"\")\n",
    "                                # Add more fields as required\n",
    "                            }\n",
    "                            \n",
    "                            # # Extracting matching lemmas\n",
    "                            matching_tokens = []\n",
    "                            matching_lemmas = []\n",
    "                            for node in xmi_data.get(\"syntax:ConllDependencyNode\", []):\n",
    "                                if int(mention.get(\"@begin\", \"\")) <= int(node.get(\"@begin\", \"\")) and int(mention.get(\"@end\", \"\")) >= int(node.get(\"@end\", \"\")):\n",
    "                                    matching_tokens.append(node.get(\"@form\", \"\"))\n",
    "                                    matching_lemmas.append(node.get(\"@lemma\", \"\"))\n",
    "                            mention_info[\"token\"] = \" \".join(matching_tokens)\n",
    "                            mention_info[\"lemma\"] = \" \".join(matching_lemmas)\n",
    "                            \n",
    "                            # Extracting matching parts of speech\n",
    "                            matching_POS = []\n",
    "                            for node in xmi_data.get(\"syntax:WordToken\", []):\n",
    "                                if int(mention.get(\"@begin\", \"\")) <= int(node.get(\"@begin\", \"\")) and int(mention.get(\"@end\", \"\")) >= int(node.get(\"@end\", \"\")):\n",
    "                                    matching_POS.append(node.get(\"@partOfSpeech\", \"\"))\n",
    "                            mention_info[\"partOfSpeech\"] = \" \".join(matching_POS)\n",
    "                            \n",
    "                            # Extract CUI and TUI from the ontology concept array\n",
    "                            ontology_ids = mention.get(\"@ontologyConceptArr\", \"\")\n",
    "                            if ontology_ids:\n",
    "                                if \" \" in ontology_ids:\n",
    "                                    ontology_ids = ontology_ids.split()\n",
    "                                else:\n",
    "                                    ontology_ids = [ontology_ids]\n",
    "                                \n",
    "                                # Loop through each ontology ID\n",
    "                                for ontology_id in ontology_ids:\n",
    "                                    umls_info = umls_concepts.get(ontology_id, {})\n",
    "                                    mention_info[\"cui\"] = umls_info.get(\"cui\", \"\")\n",
    "                                    mention_info[\"tui\"] = umls_info.get(\"tui\", \"\")\n",
    "                                    mention_info[\"preferredText\"] = umls_info.get(\"preferredText\", \"\")\n",
    "                            \n",
    "                                document_parser['clinical_mention'][mention_type].append(mention_info)\n",
    "                            \n",
    "                    elif isinstance(mentions_list, dict):  # In case there is only one mention and it's not in a list\n",
    "                        mention_info = {\n",
    "                            \"beginOffset\": mentions_list.get(\"@begin\", \"\"),\n",
    "                            \"endOffset\": mentions_list.get(\"@end\", \"\"),\n",
    "                            \"ontologyConceptArr\": mentions_list.get(\"@ontologyConceptArr\", \"\"),\n",
    "                            \"confidence\": mentions_list.get(\"@confidence\",\"\"),\n",
    "                            \"polarity\": mentions_list.get(\"@polarity\",\"\")\n",
    "                        }\n",
    "                        # Extracting matching lemmas\n",
    "                        matching_tokens = []\n",
    "                        matching_lemmas = []\n",
    "                        for node in xmi_data.get(\"syntax:ConllDependencyNode\", []):\n",
    "                            if int(mentions_list.get(\"@begin\", \"\")) <= int(node.get(\"@begin\", \"\")) and int(mentions_list.get(\"@end\", \"\")) >= int(node.get(\"@end\", \"\")):\n",
    "                                matching_tokens.append(node.get(\"@form\", \"\"))\n",
    "                                matching_lemmas.append(node.get(\"@lemma\", \"\"))\n",
    "                        mention_info[\"token\"] = \" \".join(matching_tokens)\n",
    "                        mention_info[\"lemma\"] = \" \".join(matching_lemmas)\n",
    "                        \n",
    "                        # Extracting matching parts of speech\n",
    "                        matching_POS = []\n",
    "                        for node in xmi_data.get(\"syntax:WordToken\", []):\n",
    "                            if int(mentions_list.get(\"@begin\", \"\")) <= int(node.get(\"@begin\", \"\")) and int(mentions_list.get(\"@end\", \"\")) >= int(node.get(\"@end\", \"\")):\n",
    "                                matching_POS.append(node.get(\"@partOfSpeech\", \"\"))\n",
    "                        mention_info[\"partOfSpeech\"] = \" \".join(matching_POS)\n",
    "                        \n",
    "                        # Extract CUI and TUI from the ontology concept array\n",
    "                        ontology_ids = mentions_list.get(\"@ontologyConceptArr\", \"\")\n",
    "                        if ontology_ids:\n",
    "                            if \" \" in ontology_ids:\n",
    "                                ontology_ids = ontology_ids.split()\n",
    "                            else:\n",
    "                                ontology_ids = [ontology_ids]\n",
    "                            # Loop through each ontology ID\n",
    "                            for ontology_id in ontology_ids:\n",
    "                                # print(ontology_id)\n",
    "                                umls_info = umls_concepts.get(ontology_id, {})\n",
    "                                mention_info[\"cui\"] = umls_info.get(\"cui\", \"\")\n",
    "                                mention_info[\"tui\"] = umls_info.get(\"tui\", \"\")\n",
    "                                mention_info[\"preferredText\"] = umls_info.get(\"preferredText\", \"\")\n",
    "                            \n",
    "                            document_parser['clinical_mention'][mention_type].append(mention_info)\n",
    "                        \n",
    "        # Save the parsed data to the destination directory\n",
    "        output_file_path = os.path.join(destination_dir, file_name)\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            json.dump(document_parser, output_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine into single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "\n",
    "# source_dir = \"./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/json_output/parsed\"  # Replace with your directory path\n",
    "# output_file = \"./i2b2_2010_VA_concept_assertion_relation_training_data/i2b2_text_reports_cTAKES_processing/cTAKES_output/i2b2_text_parsed_concepts_cTAKES.json\"\n",
    "    \n",
    "# combined_data = []\n",
    "\n",
    "# # Iterate over each file in the source directory\n",
    "# for file_name in os.listdir(source_dir):\n",
    "#     file_path = os.path.join(source_dir, file_name)\n",
    "\n",
    "#     # Check if the file is a .json file\n",
    "#     if os.path.isfile(file_path):\n",
    "#         with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "#             data = json.load(json_file)\n",
    "#             combined_data.append(data)\n",
    "\n",
    "# # Write the combined data to the output file\n",
    "# with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#     json.dump(combined_data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Define the directory containing your JSON files\n",
    "json_dir = '.\\i2b2_2010_VA_training_data\\i2b2_text_reports_cTAKES_processing\\json_output\\parsed'\n",
    "\n",
    "# Initialize a list to hold data for each file\n",
    "data_list = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(json_dir):\n",
    "    if filename.endswith('.txt'):  # Check if the file is a JSON file\n",
    "        file_path = os.path.join(json_dir, filename)\n",
    "        \n",
    "        # Open and load the JSON file\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "            # Extract UUID\n",
    "            uuid = data[\"UUID\"][0]  # each JSON file contains only one UUID\n",
    "            \n",
    "            # Initialize lists to collect CUIs, TUIs, and tokens\n",
    "            cuis, tuis, tokens = [], [], []\n",
    "            \n",
    "            # Loop through each mention type and collect the required information\n",
    "            for mention_type, mentions in data[\"clinical_mention\"].items():\n",
    "                for mention in mentions:\n",
    "                    cuis.append(mention[\"cui\"])\n",
    "                    tuis.append(mention[\"tui\"])\n",
    "                    tokens.append(mention[\"token\"].strip())  # Stripping any leading/trailing whitespaces\n",
    "            \n",
    "            # Append the extracted information to the data list\n",
    "            data_list.append({\n",
    "                \"File_name\": uuid,\n",
    "                \"Tokens\": tokens,\n",
    "                \"CUIs\": cuis,\n",
    "                \"TUIs\": tuis,\n",
    "            })\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_name</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>CUIs</th>\n",
       "      <th>TUIs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>018636330_DH</td>\n",
       "      <td>[DIS, Report Status, DIAGNOSIS, HISTORY, ILLNE...</td>\n",
       "      <td>[C1444662, C0586177, C0011900, C0262926, C0221...</td>\n",
       "      <td>[T033, T033, T033, T033, T184, T033, T184, T18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>026350193_RWH</td>\n",
       "      <td>[DIS, Report Status, service, Discharge Status...</td>\n",
       "      <td>[C1444662, C0586177, C0557854, C0586514, C0277...</td>\n",
       "      <td>[T033, T033, T057, T033, T033, T033, T184, T18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>037945397_RWH</td>\n",
       "      <td>[DIS, Report Status, Yes, Discharge Status, Co...</td>\n",
       "      <td>[C1444662, C0586177, C1298907, C0586514, C0277...</td>\n",
       "      <td>[T033, T033, T033, T033, T033, T184, T184, T18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>044687343_ELMVH</td>\n",
       "      <td>[DIS, Report Status, Service, YES, Shortness o...</td>\n",
       "      <td>[C1444662, C0586177, C0557854, C1298907, C0013...</td>\n",
       "      <td>[T033, T033, T057, T033, T184, T044, T184, T18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>060376519_DH</td>\n",
       "      <td>[DIS, Report Status, Discharge Status, Complai...</td>\n",
       "      <td>[C1444662, C0586177, C0586514, C0277786, C0012...</td>\n",
       "      <td>[T033, T033, T033, T033, T184, T184, T033, T04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>record-80</td>\n",
       "      <td>[Date of Birth, Birth, Sex, Service, HISTORY, ...</td>\n",
       "      <td>[C0421451, C0005615, C0009253, C0557854, C0262...</td>\n",
       "      <td>[T033, T040, T040, T057, T033, T040, T033, T03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>record-81</td>\n",
       "      <td>[Date of Birth, Birth, Sex, Service, HISTORY, ...</td>\n",
       "      <td>[C0421451, C0005615, C0009253, C0557854, C0262...</td>\n",
       "      <td>[T033, T040, T040, T057, T033, T184, T033, T03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>record-82</td>\n",
       "      <td>[Date of Birth, Birth, Sex, Service, HISTORY, ...</td>\n",
       "      <td>[C0421451, C0005615, C0009253, C0557854, C0262...</td>\n",
       "      <td>[T033, T040, T040, T057, T033, T184, T033, T03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>record-83</td>\n",
       "      <td>[Date of Birth, Birth, Sex, Service, Allergies...</td>\n",
       "      <td>[C0421451, C0005615, C0009253, C0557854, C0020...</td>\n",
       "      <td>[T033, T040, T040, T057, T046, T033, T033, T03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>record-84</td>\n",
       "      <td>[Date of Birth, Birth, Sex, Service, HISTORY, ...</td>\n",
       "      <td>[C0421451, C0005615, C0009253, C0557854, C0262...</td>\n",
       "      <td>[T033, T040, T040, T057, T033, T184, T033, T03...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           File_name                                             Tokens  \\\n",
       "0       018636330_DH  [DIS, Report Status, DIAGNOSIS, HISTORY, ILLNE...   \n",
       "1      026350193_RWH  [DIS, Report Status, service, Discharge Status...   \n",
       "2      037945397_RWH  [DIS, Report Status, Yes, Discharge Status, Co...   \n",
       "3    044687343_ELMVH  [DIS, Report Status, Service, YES, Shortness o...   \n",
       "4       060376519_DH  [DIS, Report Status, Discharge Status, Complai...   \n",
       "..               ...                                                ...   \n",
       "165        record-80  [Date of Birth, Birth, Sex, Service, HISTORY, ...   \n",
       "166        record-81  [Date of Birth, Birth, Sex, Service, HISTORY, ...   \n",
       "167        record-82  [Date of Birth, Birth, Sex, Service, HISTORY, ...   \n",
       "168        record-83  [Date of Birth, Birth, Sex, Service, Allergies...   \n",
       "169        record-84  [Date of Birth, Birth, Sex, Service, HISTORY, ...   \n",
       "\n",
       "                                                  CUIs  \\\n",
       "0    [C1444662, C0586177, C0011900, C0262926, C0221...   \n",
       "1    [C1444662, C0586177, C0557854, C0586514, C0277...   \n",
       "2    [C1444662, C0586177, C1298907, C0586514, C0277...   \n",
       "3    [C1444662, C0586177, C0557854, C1298907, C0013...   \n",
       "4    [C1444662, C0586177, C0586514, C0277786, C0012...   \n",
       "..                                                 ...   \n",
       "165  [C0421451, C0005615, C0009253, C0557854, C0262...   \n",
       "166  [C0421451, C0005615, C0009253, C0557854, C0262...   \n",
       "167  [C0421451, C0005615, C0009253, C0557854, C0262...   \n",
       "168  [C0421451, C0005615, C0009253, C0557854, C0020...   \n",
       "169  [C0421451, C0005615, C0009253, C0557854, C0262...   \n",
       "\n",
       "                                                  TUIs  \n",
       "0    [T033, T033, T033, T033, T184, T033, T184, T18...  \n",
       "1    [T033, T033, T057, T033, T033, T033, T184, T18...  \n",
       "2    [T033, T033, T033, T033, T033, T184, T184, T18...  \n",
       "3    [T033, T033, T057, T033, T184, T044, T184, T18...  \n",
       "4    [T033, T033, T033, T033, T184, T184, T033, T04...  \n",
       "..                                                 ...  \n",
       "165  [T033, T040, T040, T057, T033, T040, T033, T03...  \n",
       "166  [T033, T040, T040, T057, T033, T184, T033, T03...  \n",
       "167  [T033, T040, T040, T057, T033, T184, T033, T03...  \n",
       "168  [T033, T040, T040, T057, T046, T033, T033, T03...  \n",
       "169  [T033, T040, T040, T057, T033, T184, T033, T03...  \n",
       "\n",
       "[170 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('.\\i2b2_2010_VA_training_data\\i2b2_text_reports_cTAKES_processing\\cTAKES_output\\i2b2_text_parsed_concepts_cTAKES.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
